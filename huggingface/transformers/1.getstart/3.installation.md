安装
===
为你正在使用的深度学习库安装Transformers，设置你的缓存，并选择配置Transformers，可以离线运行。

Transformers在Python 3.6+、PyTorch 1.1.0+、TensorFlow 2.0+和Flax上测试。根据你所使用的深度学习库，安装相应的学习框架：
> * PyTorch的安装说明。
> * TensorFlow的安装说明。
> * Flax的安装说明。

## 用pip安装
你应该在虚拟环境中安装Transformers。如果你对 Python 虚拟环境不熟悉，可以看看这个指南。虚拟环境使管理不同的项目更容易，并避免依赖关系之间的兼容性问题。

首先，在你的项目目录下创建一个虚拟环境：

```python -m venv .env```  

激活虚拟环境。在Linux和MacOs上：

```source .env/bin/activate```

激活虚拟环境。在Windows上：

```.env\Scripts\activate.bat```

现在你准备用以下命令安装Transformers：

```pip install transformers```

对于只支持CPU的情况，你可以方便地在一行中安装Transformers和一个深度学习库。例如，用以下方式安装Transformers和PyTorch：

```pip install transformers[torch]```

Transformers和TensorFlow 2.0：

```pip install transformers[tf-cpu]```

Transformers和Flax：

```pip install transformers[flax]```

最后，通过运行以下命令检查Transformers是否已经正确安装。它将下载一个预训练的模型：

```python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"```

然后打印出标签和分数：

```[{'label': 'POSITIVE', 'score': 0.9998704791069031}]```

## 从源代码安装

用以下命令从源代码安装Transformers：

```pip install git+https://github.com/huggingface/transformers```

这个命令安装的是最先进的主版本，而不是最新的稳定版本。主版本对于保持最新的发展非常有用。例如，如果一个错误在上一个官方版本之后被修复了，但新的版本还没有推出来。然而，这意味着主版本可能并不总是稳定的。我们努力保持主版本的运行，大多数问题通常会在几小时或一天内得到解决。如果你遇到问题，请创建一个Issue，这样我们就能更快地解决它!

通过运行以下命令检查Transformers是否已经正确安装：

```python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"```

## 可编辑的安装

如果你愿意，你将需要一个可编辑的安装：
>* 使用源代码的主要版本。
>* 为Transformers做贡献，需要测试代码中的变化。

克隆资源库，用以下命令安装Transformers：

```
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .
```

这些命令将链接你克隆的版本库的文件夹和你的 Python 库路径。现在，除了正常的库路径之外，Python 还会在你克隆的文件夹中寻找。例如，如果你的Python包通常安装在~/anaconda3/envs/main/lib/python3.7/site-packages/，Python也会搜索你克隆到的文件夹：~/transformers/。

> 如果你想继续使用这个库，你必须保留Transformers文件夹。

现在，你可以通过以下命令轻松地将你的克隆体更新到最新版本的Transformers：

```
cd ~/transformers/
git pull
```

你的Python环境将在下次运行时找到主版本的Transformers。

## 用conda安装

从conda频道huggingface安装：

```conda install -c huggingface transformers```

## 缓存设置

预训练的模型被下载并在本地缓存：~/.cache/huggingface/hub。这是 shell 环境变量 TRANSFORMERS_CACHE 所给出的默认目录。在 Windows 上，默认缓存目录是 C:\Users\username\.cache\huggingface\hub 。你可以改变下面显示的shell环境变量--按优先顺序--来指定一个不同的缓存目录：
1. Shell环境变量（默认）：HUGGINGFACE_HUB_CACHE 或 TRANSFORMERS_CACHE。
2. Shell环境变量：HF_HOME。
3. Shell环境变量：XDG_CACHE_HOME + /huggingface。

> 如果你来自这个库的早期迭代并设置了 shell 环境变量 PYTORCH_TRANSFORMERS_CACHE 或 PYTORCH_PRETRAINED_BERT_CACHE , Transformers将使用这两个变量，除非你指定shell环境变量 TRANSFORMERS_CACHE。

## 离线模式

Transformers能够通过只使用本地文件在防火墙或离线环境中运行。设置环境变量TRANSFORMERS_OFFLINE=1来启用这种行为。

> 通过设置环境变量 HF_DATASETS_OFFLINE=1，将 Datasets 添加到你的离线训练工作流。

例如，你通常会用以下命令在对外部实例进行防火墙的正常网络上运行一个程序：

```python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...```

在一个离线实例中运行这个相同的程序，用：

```
HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

脚本现在应该不会挂起或等待超时，因为它知道它应该只寻找本地文件。

### 获取模型和标记器，以便在离线情况下使用

另一种离线使用Transformers的方法是提前下载文件，然后在需要离线使用时指向其本地路径。有三种方法可以做到这一点：

>* 通过Model Hub上的用户界面，点击↓图标下载文件。
>![图片](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png)
>
>* 使用PreTrainedModel.from_pretrained（）和PreTrainedModel.save_pretrained（）进行工作流程：
>1. 用PreTrainedModel.from_pretrained()提前下载你的文件：
>```
>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
>
>tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
>model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")
>```
>2. 用PreTrainedModel.save_pretrained()将你的文件保存到指定的目录中：
>```
>tokenizer.save_pretrained("./your/path/bigscience_t0")
>model.save_pretrained("./your/path/bigscience_t0")
>```
>3. 现在，当你离线时，用PreTrainedModel.from_pretrained()从指定目录中重新加载你的文件：
>```
>tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
>model = AutoModel.from_pretrained("./your/path/bigscience_t0")
>```
>* 用huggingface_hub库以编程方式下载文件：
>1. 在你的虚拟环境中安装huggingface_hub库：
>```
>python -m pip install huggingface_hub
>```
>2. 使用hf_hub_download函数来下载一个文件到一个特定的路径。例如，下面的命令将T0模型中的config.json文件下载到你想要的路径：
>```
>from huggingface_hub import hf_hub_download
>
>hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
>```

一旦你的文件被下载并在本地缓存，指定它的本地路径来加载和使用它：

```
from transformers import AutoConfig

config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

>关于下载存储在Hub上的文件的更多细节，请参见[如何从Hub上下载文件](https://huggingface.co/docs/huggingface_hub/v0.15.1/guides/download)一节。