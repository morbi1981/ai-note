用Pipelines进行推理
=================

使用pipeline()对Hub中的模型变得非常简单，这些推理可以是任何语言、计算机视觉、语音和多模态任务。即使你没有特定模式的经验，或者不熟悉模型背后的底层代码，你仍然可以用pipeline()将它们用于推理！本教程将教你：

* 使用pipeline()进行推理。
* 使用一个特定的标记器或模型。
* 对音频、视觉和多模态任务使用pipeline()进行推理。

> 看看pipeline()文档，了解支持的任务和可用参数的完整列表。

## pipeline()的使用

虽然每个任务都有一个相关的pipeline()，但使用一般的pipeline()抽象更简单，它包含所有特定任务的管道。pipeline()为你的任务自动加载一个默认模型和一个能够进行推理的预处理类。

1. 首先创建一个pipeline()并指定一个推理任务：
```
from transformers import pipeline

generator = pipeline(task="automatic-speech-recognition")
```
2. 将你的输入文本传递给pipeline()：

```
generator("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```
```
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}
```

不是你想象中的结果？看看Hub上一些下载量最大的自动语音识别模型，看看你是否能得到更好的转录。让我们试试openai/whisper-large：

```
generator = pipeline(model="openai/whisper-large")
generator("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```
```
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

现在，这个结果看起来更准确了!我们真的鼓励你查看Hub，以了解不同语言的模型、你所在领域的专业模型，以及更多的相关知识。你可以直接从你的浏览器上查看和比较Hub上的模型结果，看看它是否比其他的模型更适合或处理细节的情况。如果你没有找到适合你的用例的模型，你还可以训练你自己的模型!

如果你有几个输入，你可以把你的输入作为一个列表传递：
    
```
generator(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
)
```

如果你想对整个数据集进行迭代，或者想在网络服务器中使用它进行推理，请查看专用部分

在一个数据集上使用pipelines

为网络服务器使用pipelines

## 参数

pipeline()支持许多参数；有些是特定的任务，有些是所有管道的通用参数。一般来说，你可以在任何你想要的地方指定参数：

```
generator = pipeline(model="openai/whisper-large", my_parameter=1)
out = generator(...)  # This will use `my_parameter=1`.
out = generator(..., my_parameter=2)  # This will override and use `my_parameter=2`.
out = generator(...)  # This will go back to using `my_parameter=1`.
```

让我们来看看3个重要的参数：

### Device

如果你使用device=n，pipeline会自动将模型放在指定的设备上。无论你使用的是PyTorch还是Tensorflow，这都将发挥作用。

```
generator = pipeline(model="openai/whisper-large", device=0)
```

如果模型对单个GPU来说太大，你可以设置device_map="auto"，让Accelerate自动决定如何加载和存储模型权重。

```
#!pip install accelerate
generator = pipeline(model="openai/whisper-large", device_map="auto")
```

注意，如果device_map="auto "被通过，那么在实例化你的管道时就没有必要添加参数device=device，否则你可能会遇到一些意想不到的行为！

### Batch size

默认情况下，pipeline不会批量推理，原因在此详细解释。原因是，批处理不一定更快，实际上在某些情况下可能会相当慢。

但如果它在你的用例中有效，你可以使用：

```
generator = pipeline(model="openai/whisper-large", device=0, batch_size=2)
audio_filenames = [f"audio_{i}.flac" for i in range(10)]
texts = generator(audio_filenames)
```

这是在pipeline处理10个音频文件，但它会将这些文件以2个为一个批次传递给模型（它在GPU上，在那里批处理更可能有帮助），而不需要你再写任何代码。输出应该总是与你在没有批处理的情况下收到的内容一致。它只是作为一种方法，帮助你从pipeline中获得更多的速度。

pipeline还可以减轻批处理的一些复杂性，因为对于一些管道来说，一个单一的项目（比如一个长的音频文件）需要被分成多个部分，由一个模型来处理。管道为你执行这种分块批处理工作。

### 特定任务参数

所有任务都提供特定的任务参数，允许额外的灵活性和选项，以帮助你完成你的工作。例如，transformers.AutomaticSpeechRecognitionPipeline.call()方法有一个return_timestamps参数，对于视频字幕制作很有帮助：

```
# Not using whisper, as it cannot provide timestamps.
generator = pipeline(model="facebook/wav2vec2-large-960h-lv60-self", return_timestamps="word")
generator("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```
```
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP AND LIVE OUT THE TRUE MEANING OF ITS CREED', 'chunks': [{'text': 'I', 'timestamp': (1.22, 1.24)}, {'text': 'HAVE', 'timestamp': (1.42, 1.58)}, {'text': 'A', 'timestamp': (1.66, 1.68)}, {'text': 'DREAM', 'timestamp': (1.76, 2.14)}, {'text': 'BUT', 'timestamp': (3.68, 3.8)}, {'text': 'ONE', 'timestamp': (3.94, 4.06)}, {'text': 'DAY', 'timestamp': (4.16, 4.3)}, {'text': 'THIS', 'timestamp': (6.36, 6.54)}, {'text': 'NATION', 'timestamp': (6.68, 7.1)}, {'text': 'WILL', 'timestamp': (7.32, 7.56)}, {'text': 'RISE', 'timestamp': (7.8, 8.26)}, {'text': 'UP', 'timestamp': (8.38, 8.48)}, {'text': 'AND', 'timestamp': (10.08, 10.18)}, {'text': 'LIVE', 'timestamp': (10.26, 10.48)}, {'text': 'OUT', 'timestamp': (10.58, 10.7)}, {'text': 'THE', 'timestamp': (10.82, 10.9)}, {'text': 'TRUE', 'timestamp': (10.98, 11.18)}, {'text': 'MEANING', 'timestamp': (11.26, 11.58)}, {'text': 'OF', 'timestamp': (11.66, 11.7)}, {'text': 'ITS', 'timestamp': (11.76, 11.88)}, {'text': 'CREED', 'timestamp': (12.0, 12.38)}]}
```

正如你所看到的，该模型推断出了文本，还输出了句子中各个单词的发音时间。

每个任务都有很多参数，所以请查看每个任务的API参考，看看能使用这些参数做些什么特殊工作。例如，AutomaticSpeechRecognitionPipeline有一个chunk_length_s参数，有助于处理真正长的音频文件（例如，为整部电影或一小时长的视频做字幕），而模型通常无法自行处理。

如果你找不到真正对你有帮助的参数，请随时提出要求！

## 在一个数据集上使用pipelines

pipeline还可以在大型数据集上运行推理。我们建议最简单的方法是使用一个迭代器：

```
def data():
    for i in range(1000):
        yield f"My example {i}"


pipe = pipeline(model="gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out[0]["generated_text"])
```

迭代器data()产生每个结果，pipeline自动识别输入是可迭代的，并将开始获取数据，同时继续在GPU上处理它（这在引擎中使用了DataLoader）。这一点很重要，因为你不必为整个数据集分配内存，你可以尽可能快地供给GPU。

由于批处理可以加速事情的推理，在这里尝试调整batch_size参数可能是有用的。

迭代数据集的最简单方法是直接从Datasets中加载一个：

```
# KeyDataset is a util that will just output the item we're interested in.
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

## 为网络服务器使用pipeline

> 创建一个推理引擎是一个复杂的话题，应该有自己的页面。参考下面链接。

[网络连接](https://huggingface.co/docs/transformers/pipeline_webserver)

## 视觉任务 pipeline

为视觉任务使用 pipeline()实际上是相同的。

指定你的任务并把你的图像传给分类器。该图像可以是一个链接或图像的本地路径。例如，下面显示的是什么品种的猫？

![猫](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg)

```
from transformers import pipeline

vision_classifier = pipeline(model="google/vit-base-patch16-224")
preds = vision_classifier(
    images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
)
preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
preds
```
```
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

## 文本任务 pipeline

为NLP任务使用pipeline()实际上是相同的。

```
from transformers import pipeline

# This model is a `zero-shot-classification` model.
# It will classify text, except you are free to choose any label you might imagine
classifier = pipeline(model="facebook/bart-large-mnli")
classifier(
    "I have a problem with my iphone that needs to be resolved asap!!",
    candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
)
```
```
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}
```

## 多式联运 pipeline

pipeline()支持一种以上的模式。例如，一个视觉问题回答（VQA）任务结合了文本和图像。你可以随意使用你喜欢的任何图片链接和你想问的关于图片的问题。图片可以是一个URL或图片的本地路径。

例如，如果你使用这张发票图片：

```
from transformers import pipeline

vqa = pipeline(model="impira/layoutlm-document-qa")
vqa(
    image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
    question="What is the invoice number?",
)
```
```
[{'score': 0.42515, 'answer': 'us-001', 'start': 16, 'end': 16}]
```

> 要运行上面的例子，除了Transformers，你还需要安装pytesseract：
> ```
> sudo apt install -y tesseract-ocr
> pip install pytesseract
> ```
>

## 使用pipeline在大型模型上通过accelerate加速：

你可以使用accelerate轻松地在大型模型上运行管道!首先，确保你已经用 ```pip install accelerate``` 安装了加速器。

首先使用 ```device_map="auto "``` 加载你的模型!我们将使用 ```facebook/opt-1.3b``` 作为我们的例子。

```
# pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

如果你安装了```bitsandbytes```并添加参数```load_in_8bit=True```，你也可以传递8位加载的模型。

```
# pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

请注意，你可以用任何一个支持大型模型加载的Hugging Face模型（如BLOOM！）取代检查点！